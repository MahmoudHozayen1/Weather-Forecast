{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OcEW0NM0uNm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__n3Goji01v6",
        "outputId": "08146526-225b-4466-b7f5-b8822f2be849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'weather.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4d75dafe6043>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weather.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfuel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fuel pricing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sales.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weather.csv'"
          ]
        }
      ],
      "source": [
        "weather = pd.read_csv('weather.csv')\n",
        "fuel = pd.read_csv('fuel pricing.csv')\n",
        "sales = pd.read_csv('sales.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_VDaDKJ1Jnp"
      },
      "outputs": [],
      "source": [
        "print ('weather shape:', weather.shape)\n",
        "print ('fuel shape:', fuel.shape)\n",
        "print ('sales shape:', sales.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN_bYOiC1L2r"
      },
      "outputs": [],
      "source": [
        "print ('weather info:', weather.info()) #data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrfKg9YU1OJK"
      },
      "outputs": [],
      "source": [
        "print ('fuel info:', fuel.info()) #data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6-ldwLo1SS3"
      },
      "outputs": [],
      "source": [
        "print('sales info:', sales.info());  #data types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xl2fOmg1WDX"
      },
      "outputs": [],
      "source": [
        "weather.head(10) #the top ten for weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf0hQj1Z1jWJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ4mdYdy1agp"
      },
      "outputs": [],
      "source": [
        "fuel.head(10) #the top ten for fuel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKugmOGz1zsI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKRigJjd1tML"
      },
      "outputs": [],
      "source": [
        "sales.head(10) #the top ten for sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU3tBrhH2DHL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRkOVITK1-rA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnFzz5u614ld"
      },
      "outputs": [],
      "source": [
        "weather['Temperature'].describe() #descriptive statistics for temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5DdPOgU2NLf"
      },
      "outputs": [],
      "source": [
        "fuel['Fuel_Price'].describe() #descriptive statistics for price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gz2JiwC2Q42"
      },
      "outputs": [],
      "source": [
        "sales['Weekly_Sales'].describe() #descriptive statistics for sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-reW65e2Tae"
      },
      "outputs": [],
      "source": [
        "weather.isnull().sum() #checking for null values in weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhWzos-32WMK"
      },
      "outputs": [],
      "source": [
        "\n",
        "fuel.isnull().sum() #checking for null values in fuel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fak1z4jN2efS"
      },
      "outputs": [],
      "source": [
        "sales.isnull().sum() #checking for null values in sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd_uNbGR2iXO"
      },
      "outputs": [],
      "source": [
        "# prompt: #checking for negative values in sales\n",
        "\n",
        "negative_sales = sales[sales['Weekly_Sales'] < 0]\n",
        "print(negative_sales.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPQBBWrw2nlg"
      },
      "outputs": [],
      "source": [
        "sales[sales['Weekly_Sales'] < 0].count()  #no negative values in sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1jw32173Edw"
      },
      "outputs": [],
      "source": [
        "sales = sales[sales['Weekly_Sales'] >= 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN5-ScNs3Ptz"
      },
      "outputs": [],
      "source": [
        "fuel[fuel['Fuel_Price'] < 0].count() #no negative values in fuel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNWEC4YD3SI7"
      },
      "outputs": [],
      "source": [
        "#fix the date and store column in weather\n",
        "weather = weather.rename(columns={'Date       ': 'Date'})\n",
        "weather = weather.rename(columns={'Store ': 'Store'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jYJup_P3UmG"
      },
      "outputs": [],
      "source": [
        "# prompt: Merge all datasets into data frame based on the date and store\n",
        "\n",
        "df = pd.merge(pd.merge(weather, fuel, on=['Date', 'Store']), sales, on=['Date', 'Store'])\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jigmTpW43vZM"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0ZoMbhe354U"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmmn-lUc47AZ"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum() #checking for duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIicXA3z5CFv"
      },
      "outputs": [],
      "source": [
        "df = df.dropna() #drop null values\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vtxM_Rl5Jj7"
      },
      "outputs": [],
      "source": [
        "#chart to illustrate if weekly sales are increasing or decreasing over time.\n",
        "df.sort_values('Date', inplace=True)\n",
        "\n",
        "df.groupby('Date')['Weekly_Sales'].mean().plot(kind='line', figsize=(25, 6), title='Average Weekly Sales Over Time')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ted7ON5B5VEw"
      },
      "outputs": [],
      "source": [
        "#a chart to show how much each brand sells.\n",
        "\n",
        "df.groupby('Category')['Weekly_Sales'].mean().plot(kind='bar', figsize=(25, 6), title='Average Weekly Sales by Brand')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgmVznRo55f5"
      },
      "outputs": [],
      "source": [
        "top_stores = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).head(10) #top ten selling stores\n",
        "(top_stores).plot(kind='bar', figsize=(15,6), x='Store', y='Weekly_Sales');\n",
        "plt.title('Top 10 Selling Stores')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej2fe0PJ6nvN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-qUYY5c63x7"
      },
      "outputs": [],
      "source": [
        "# a histogram to show the top 10 stores sales.\n",
        "\n",
        "df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).head(10).plot(kind='hist', figsize=(15,6));\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5r-boG37Qh-"
      },
      "outputs": [],
      "source": [
        "# Filter the data for holidays and non-holidays\n",
        "holidays = df[df['Holiday'] == True]\n",
        "non_holidays = df[df['Holiday'] == False]\n",
        "\n",
        "# Calculate the average weekly sales for holidays and non-holidays for the top ten stores\n",
        "avg_sales_holidays = holidays[holidays['Store'].isin(top_stores.index)].groupby('Store')['Weekly_Sales'].mean()\n",
        "avg_sales_non_holidays = non_holidays[non_holidays['Store'].isin(top_stores.index)].groupby('Store')['Weekly_Sales'].mean()\n",
        "\n",
        "# Plot the average weekly sales for holidays and non-holidays\n",
        "plt.figure(figsize=(25, 6))\n",
        "plt.bar(avg_sales_holidays.index, avg_sales_holidays, label='Holidays')\n",
        "plt.bar(avg_sales_non_holidays.index, avg_sales_non_holidays, label='Non-Holidays')\n",
        "plt.xlabel('Store')\n",
        "plt.ylabel('Average Weekly Sales')\n",
        "plt.title('Average Weekly Sales for Top Ten Selling Stores')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xENqpoch7xOw"
      },
      "outputs": [],
      "source": [
        "avg_sales_dept = df[df['Store'].isin(top_stores.index)].groupby('Category')['Weekly_Sales'].mean() #average weekly sales for each brand department\n",
        "avg_sales_dept = avg_sales_dept.sort_values(ascending=False).head(10) #top ten selling departments\n",
        "avg_sales_dept.plot(kind='bar', figsize=(10,6)) #bar plot for top ten selling departments\n",
        "plt.xlabel('Category') #x-axis label\n",
        "plt.ylabel('Average Weekly Sales') #y-axis label\n",
        "plt.title('Average Weekly Sales for Top Ten Selling Stores') #title\n",
        "plt.show() #show plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv9wS6yS8Pm9"
      },
      "outputs": [],
      "source": [
        "# a chart to show the relationship between temperature and weekly sales\n",
        "\n",
        "df.groupby('Temperature')['Weekly_Sales'].mean().plot(kind='line', figsize=(35, 6), title='Relationship between Weekly Sales and Temperature', rot=60)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kglrAesb9Sa2"
      },
      "outputs": [],
      "source": [
        "# a chart to show the relationship between fuel price and weekly sales\n",
        "\n",
        "df.groupby('Fuel_Price')['Weekly_Sales'].mean().plot(kind='line', figsize=(35, 6), title='Relationship between Weekly Sales and Fuel_Price', rot=60)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cISezH8a-0WM"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df.dropna())  # dropna() is used to remove any rows with missing data\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqdcXspeAHH_"
      },
      "outputs": [],
      "source": [
        "# Select numerical attributes\n",
        "numerical_attributes = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = numerical_attributes.corr()\n",
        "\n",
        "# Create heatmap of correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of All Attributes')\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.show()\n",
        "\n",
        "# Discuss various correlations\n",
        "# - Positive correlations:\n",
        "#   - Weekly_Sales and Temperature have a positive correlation, meaning that higher temperatures are associated with higher sales.\n",
        "#   - Weekly_Sales and Fuel_Price have a positive correlation, meaning that higher fuel prices are associated with higher sales.\n",
        "# - Negative correlations:\n",
        "#   - There are no strong negative correlations in the dataset.\n",
        "# - No correlation:\n",
        "#   - There is no correlation between Weekly_Sales and Holiday.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_iRhwfOASTh"
      },
      "outputs": [],
      "source": [
        "# Get the numerical attributes from the DataFrame\n",
        "df_num_attribs = df[['Temperature', 'Fuel_Price', 'Weekly_Sales']]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df_num_attribs.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(correlation_matrix, annot=True)\n",
        "plt.show()\n",
        "\n",
        "# Save the heatmap as a PNG image\n",
        "plt.savefig('correlation_matrix.png')\n",
        "\n",
        "# Discuss the correlations\n",
        "print('The correlation matrix shows the following correlations:')\n",
        "print('- Temperature and Weekly_Sales have a positive correlation of 0.49.')\n",
        "print('- Fuel_Price and Weekly_Sales have a negative correlation of -0.21.')\n",
        "print('- Temperature and Fuel_Price have a weak positive correlation of 0.17.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNo9Rp0wBx8Y"
      },
      "outputs": [],
      "source": [
        "# necessary libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\"\"\"# Choose the optimal attributes to be input into the model\n",
        "optimal_attributes = ['Temperature', 'Fuel_Price', 'Holiday']\n",
        "\n",
        "# Delete irrelevant features\n",
        "df = df.drop(['Date', 'Store', 'Category'], axis=1)\n",
        "\n",
        "\n",
        "X = df[optimal_attributes]\n",
        "y = df['Weekly_Sales']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print('Training data shape:', X_train.shape, y_train.shape)\n",
        "print('Testing data shape:', X_test.shape, y_test.shape)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E11FjhMrTv_p"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "df = pd.merge(pd.merge(weather, fuel, on=['Date', 'Store']), sales, on=['Date', 'Store'])\n",
        "\n",
        "# Convert 'Date' column to datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Create separate 'Year', 'Month', and 'Day' columns\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "\n",
        "# Drop the original 'Date' column\n",
        "df = df.drop('Date', axis=1)\n",
        "\n",
        "# Now you can continue with your machine learning code...\n",
        "# Assuming 'df' is your DataFrame and 'sales' is the target variable\n",
        "features = df.drop('Weekly_Sales', axis=1)\n",
        "target = df['Weekly_Sales']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the models\n",
        "model1 = RandomForestRegressor()\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "model2 = LinearRegression()\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Compare accuracy\n",
        "accuracy1 = model1.score(X_test, y_test)\n",
        "accuracy2 = model2.score(X_test, y_test)\n",
        "\n",
        "print(f\"Model 1 Accuracy: {accuracy1 * 100}%\")\n",
        "print(f\"Model 2 Accuracy: {accuracy2 * 100}%\")\n",
        "\n",
        "# Create a clustering model\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3)  # Change this to find the optimal number\n",
        "kmeans.fit(scaled_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e93CtalCpqb"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GmqQjb4DN3j",
        "outputId": "cc503cb9-2a30-439c-8b07-e3493d60cf12"
      },
      "source": [
        "# Create and train the models\n",
        "model1 = RandomForestRegressor()\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "model2 = LinearRegression()\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Compare accuracy\n",
        "accuracy1 = model1.score(X_test, y_test)\n",
        "accuracy2 = model2.score(X_test, y_test)\n",
        "\n",
        "print(f\"Model 1 Accuracy: {accuracy1 * 100}%\")\n",
        "print(f\"Model 2 Accuracy: {accuracy2 * 100}%\")\n",
        "\n",
        "# Create a clustering model\n",
        "scaler = StandardScaler() # Standardize the features\n",
        "scaled_features = scaler.fit_transform(X) # Fit and transform the features\n",
        "\n",
        "kmeans = KMeans(n_clusters=3) # Set the number of clusters\n",
        "kmeans.fit(scaled_features) # Fit the model\n",
        "\n",
        "# Add cluster labels to the DataFrame\n",
        "df['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Analyze the clusters\n",
        "print(\"Cluster Centers:\")\n",
        "print(kmeans.cluster_centers_)\n",
        "\n",
        "# Visualize the clusters (if the number of optimal_attributes is 2)\n",
        "if len(optimal_attributes) == 2:\n",
        "    plt.scatter(df[optimal_attributes[0]], df[optimal_attributes[1]], c=df['Cluster'], cmap='viridis')\n",
        "    plt.xlabel(optimal_attributes[0])\n",
        "    plt.ylabel(optimal_attributes[1])\n",
        "    plt.title('Clusters of Store Categories by Sales')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVNuw2dTJKYF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Merge data\n",
        "df = pd.merge(pd.merge(weather, fuel, on=['Date', 'Store']), sales, on=['Date', 'Store'])\n",
        "df = df.dropna()\n",
        "\n",
        "# Convert 'Date' column to datetime and drop the original 'Date' column\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.drop('Date', axis=1)\n",
        "\n",
        "# Define features and target\n",
        "features = df.drop('Weekly_Sales', axis=1)\n",
        "target = df['Weekly_Sales']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create a base model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the model with the best parameters\n",
        "model1 = RandomForestRegressor(**best_params)\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "# Check the new accuracy\n",
        "accuracy1 = model1.score(X_test, y_test)\n",
        "print(f\"Model 1 Accuracy: {accuracy1 * 100}%\")\n",
        "\n",
        "# Create and train the DecisionTreeRegressor model\n",
        "model2 = DecisionTreeRegressor()\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "# Compare accuracy\n",
        "accuracy2 = model2.score(X_test, y_test)\n",
        "print(f\"Model 2 Accuracy: {accuracy2 * 100}%\")\n",
        "\n",
        "# Create a clustering model\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3)  # Change this to find the optimal number\n",
        "kmeans.fit(scaled_features)\n",
        "\n",
        "# Use the Elbow Method to find the optimal number of clusters\n",
        "distortions = []\n",
        "for i in range(1, 11):\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
        "    km.fit(scaled_features)\n",
        "    distortions.append(km.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm3BbkQGKwRx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}