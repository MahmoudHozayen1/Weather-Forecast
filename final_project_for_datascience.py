# -*- coding: utf-8 -*-
"""Final project for datascience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hBNo8ypP5v5fyxKXWmDk2LobxZ8D0VNf
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

weather = pd.read_csv('weather.csv')
fuel = pd.read_csv('fuel pricing.csv')
sales = pd.read_csv('sales.csv')

print ('weather shape:', weather.shape)
print ('fuel shape:', fuel.shape)
print ('sales shape:', sales.shape)

print ('weather info:', weather.info()) #data types

print ('fuel info:', fuel.info()) #data types

print('sales info:', sales.info());  #data types

weather.head(10) #the top ten for weather



fuel.head(10) #the top ten for fuel



sales.head(10) #the top ten for sales





weather['Temperature'].describe() #descriptive statistics for temperature

fuel['Fuel_Price'].describe() #descriptive statistics for price

sales['Weekly_Sales'].describe() #descriptive statistics for sales

weather.isnull().sum() #checking for null values in weather

fuel.isnull().sum() #checking for null values in fuel

sales.isnull().sum() #checking for null values in sales

# prompt: #checking for negative values in sales

negative_sales = sales[sales['Weekly_Sales'] < 0]
print(negative_sales.shape)

sales[sales['Weekly_Sales'] < 0].count()  #no negative values in sales

sales = sales[sales['Weekly_Sales'] >= 0]

fuel[fuel['Fuel_Price'] < 0].count() #no negative values in fuel

#fix the date and store column in weather
weather = weather.rename(columns={'Date       ': 'Date'})
weather = weather.rename(columns={'Store ': 'Store'})

# prompt: Merge all datasets into data frame based on the date and store

df = pd.merge(pd.merge(weather, fuel, on=['Date', 'Store']), sales, on=['Date', 'Store'])
df.head()

df

df.isnull().sum()

df.duplicated().sum() #checking for duplicates

df = df.dropna() #drop null values
df

#chart to illustrate if weekly sales are increasing or decreasing over time.
df.sort_values('Date', inplace=True)

df.groupby('Date')['Weekly_Sales'].mean().plot(kind='line', figsize=(25, 6), title='Average Weekly Sales Over Time')
plt.gca().spines[['top', 'right']].set_visible(False)

#a chart to show how much each brand sells.

df.groupby('Category')['Weekly_Sales'].mean().plot(kind='bar', figsize=(25, 6), title='Average Weekly Sales by Brand')
plt.gca().spines[['top', 'right']].set_visible(False)

top_stores = df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).head(10) #top ten selling stores
(top_stores).plot(kind='bar', figsize=(15,6), x='Store', y='Weekly_Sales');
plt.title('Top 10 Selling Stores')
plt.legend();



# a histogram to show the top 10 stores sales.

df.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False).head(10).plot(kind='hist', figsize=(15,6));

# Filter the data for holidays and non-holidays
holidays = df[df['Holiday'] == True]
non_holidays = df[df['Holiday'] == False]

# Calculate the average weekly sales for holidays and non-holidays for the top ten stores
avg_sales_holidays = holidays[holidays['Store'].isin(top_stores.index)].groupby('Store')['Weekly_Sales'].mean()
avg_sales_non_holidays = non_holidays[non_holidays['Store'].isin(top_stores.index)].groupby('Store')['Weekly_Sales'].mean()

# Plot the average weekly sales for holidays and non-holidays
plt.figure(figsize=(25, 6))
plt.bar(avg_sales_holidays.index, avg_sales_holidays, label='Holidays')
plt.bar(avg_sales_non_holidays.index, avg_sales_non_holidays, label='Non-Holidays')
plt.xlabel('Store')
plt.ylabel('Average Weekly Sales')
plt.title('Average Weekly Sales for Top Ten Selling Stores')
plt.legend()
plt.show()

avg_sales_dept = df[df['Store'].isin(top_stores.index)].groupby('Category')['Weekly_Sales'].mean() #average weekly sales for each brand department
avg_sales_dept = avg_sales_dept.sort_values(ascending=False).head(10) #top ten selling departments
avg_sales_dept.plot(kind='bar', figsize=(10,6)) #bar plot for top ten selling departments
plt.xlabel('Category') #x-axis label
plt.ylabel('Average Weekly Sales') #y-axis label
plt.title('Average Weekly Sales for Top Ten Selling Stores') #title
plt.show() #show plot

# a chart to show the relationship between temperature and weekly sales

df.groupby('Temperature')['Weekly_Sales'].mean().plot(kind='line', figsize=(35, 6), title='Relationship between Weekly Sales and Temperature', rot=60)
plt.gca().spines[['top', 'right']].set_visible(False)

# a chart to show the relationship between fuel price and weekly sales

df.groupby('Fuel_Price')['Weekly_Sales'].mean().plot(kind='line', figsize=(35, 6), title='Relationship between Weekly Sales and Fuel_Price', rot=60)
plt.gca().spines[['top', 'right']].set_visible(False)

sns.pairplot(df.dropna())  # dropna() is used to remove any rows with missing data

# Display the plot
plt.show()

# Select numerical attributes
numerical_attributes = df.select_dtypes(include=['int64', 'float64'])

# Calculate correlation matrix
correlation_matrix = numerical_attributes.corr()

# Create heatmap of correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of All Attributes')
plt.savefig('correlation_matrix.png')
plt.show()

# Discuss various correlations
# - Positive correlations:
#   - Weekly_Sales and Temperature have a positive correlation, meaning that higher temperatures are associated with higher sales.
#   - Weekly_Sales and Fuel_Price have a positive correlation, meaning that higher fuel prices are associated with higher sales.
# - Negative correlations:
#   - There are no strong negative correlations in the dataset.
# - No correlation:
#   - There is no correlation between Weekly_Sales and Holiday.

# Get the numerical attributes from the DataFrame
df_num_attribs = df[['Temperature', 'Fuel_Price', 'Weekly_Sales']]

# Calculate the correlation matrix
correlation_matrix = df_num_attribs.corr()

# Create a heatmap of the correlation matrix
sns.heatmap(correlation_matrix, annot=True)
plt.show()

# Save the heatmap as a PNG image
plt.savefig('correlation_matrix.png')

# Discuss the correlations
print('The correlation matrix shows the following correlations:')
print('- Temperature and Weekly_Sales have a positive correlation of 0.49.')
print('- Fuel_Price and Weekly_Sales have a negative correlation of -0.21.')
print('- Temperature and Fuel_Price have a weak positive correlation of 0.17.')

# necessary libraries
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

"""# Choose the optimal attributes to be input into the model
optimal_attributes = ['Temperature', 'Fuel_Price', 'Holiday']

# Delete irrelevant features
df = df.drop(['Date', 'Store', 'Category'], axis=1)


X = df[optimal_attributes]
y = df['Weekly_Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('Training data shape:', X_train.shape, y_train.shape)
print('Testing data shape:', X_test.shape, y_test.shape)
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import numpy as np

df = pd.merge(pd.merge(weather, fuel, on=['Date', 'Store']), sales, on=['Date', 'Store'])

# Convert 'Date' column to datetime
df['Date'] = pd.to_datetime(df['Date'])

# Create separate 'Year', 'Month', and 'Day' columns
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

# Drop the original 'Date' column
df = df.drop('Date', axis=1)

# Now you can continue with your machine learning code...
# Assuming 'df' is your DataFrame and 'sales' is the target variable
features = df.drop('Weekly_Sales', axis=1)
target = df['Weekly_Sales']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Create and train the models
model1 = RandomForestRegressor()
model1.fit(X_train, y_train)

model2 = LinearRegression()
model2.fit(X_train, y_train)

# Compare accuracy
accuracy1 = model1.score(X_test, y_test)
accuracy2 = model2.score(X_test, y_test)

print(f"Model 1 Accuracy: {accuracy1 * 100}%")
print(f"Model 2 Accuracy: {accuracy2 * 100}%")

# Create a clustering model
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=3)  # Change this to find the optimal number
kmeans.fit(scaled_features)

df

"""# Create and train the models
model1 = RandomForestRegressor()
model1.fit(X_train, y_train)

model2 = LinearRegression()
model2.fit(X_train, y_train)

# Compare accuracy
accuracy1 = model1.score(X_test, y_test)
accuracy2 = model2.score(X_test, y_test)

print(f"Model 1 Accuracy: {accuracy1 * 100}%")
print(f"Model 2 Accuracy: {accuracy2 * 100}%")

# Create a clustering model
scaler = StandardScaler() # Standardize the features
scaled_features = scaler.fit_transform(X) # Fit and transform the features

kmeans = KMeans(n_clusters=3) # Set the number of clusters
kmeans.fit(scaled_features) # Fit the model

# Add cluster labels to the DataFrame
df['Cluster'] = kmeans.labels_

# Analyze the clusters
print("Cluster Centers:")
print(kmeans.cluster_centers_)

# Visualize the clusters (if the number of optimal_attributes is 2)
if len(optimal_attributes) == 2:
    plt.scatter(df[optimal_attributes[0]], df[optimal_attributes[1]], c=df['Cluster'], cmap='viridis')
    plt.xlabel(optimal_attributes[0])
    plt.ylabel(optimal_attributes[1])
    plt.title('Clusters of Store Categories by Sales')
    plt.show()
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Merge data
df = pd.merge(pd.merge(weather, fuel, on=['Date', 'Store']), sales, on=['Date', 'Store'])
df = df.dropna()

# Convert 'Date' column to datetime and drop the original 'Date' column
df['Date'] = pd.to_datetime(df['Date'])
df = df.drop('Date', axis=1)

# Define features and target
features = df.drop('Weekly_Sales', axis=1)
target = df['Weekly_Sales']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Create a base model
rf = RandomForestRegressor()

# Instantiate the grid search model
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best parameters
best_params = grid_search.best_params_

# Train the model with the best parameters
model1 = RandomForestRegressor(**best_params)
model1.fit(X_train, y_train)

# Check the new accuracy
accuracy1 = model1.score(X_test, y_test)
print(f"Model 1 Accuracy: {accuracy1 * 100}%")

# Create and train the DecisionTreeRegressor model
model2 = DecisionTreeRegressor()
model2.fit(X_train, y_train)

# Compare accuracy
accuracy2 = model2.score(X_test, y_test)
print(f"Model 2 Accuracy: {accuracy2 * 100}%")

# Create a clustering model
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

kmeans = KMeans(n_clusters=3)  # Change this to find the optimal number
kmeans.fit(scaled_features)

# Use the Elbow Method to find the optimal number of clusters
distortions = []
for i in range(1, 11):
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0)
    km.fit(scaled_features)
    distortions.append(km.inertia_)

plt.plot(range(1, 11), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.show()

